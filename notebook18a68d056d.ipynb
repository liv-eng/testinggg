{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Esse projeto parece ser uma ótima oportunidade para explorar Processamento de Linguagem Natural (NLP) e desenvolver habilidades práticas em ciência de dados!\nPlano de Trabalho para o Miniprojeto\nConfiguração Inicial\n- Objetivo: Configurar o ambiente e se familiarizar com o desafio no Kaggle.\n  - Crie um ambiente no Kaggle, Google Colab ou Jupyter Notebook local.\n  - Faça o download do conjunto de dados da competição.\n  - Explore a página da competição para entender os objetivos, métricas de avaliação e formato de envio.\nNotebook ou Relatório\nIntrodução\n- Breve descrição do problema e dos dados:\n  - Explique o objetivo: classificar tweets como relacionados ou não a desastres.\n  - Descreva os dados: número de entradas, dimensões, estrutura (exemplo: texto, rótulos, IDs).\n  - Discuta o papel de NLP e a relevância desse problema no mundo real.\n\nAnálise Exploratória de Dados:\n- Visualize o conjunto de dados:\n  - Verifique valores nulos e distribuições de classes.\n  - Explore estatísticas básicas como comprimento dos tweets, presença de links/hashtags, etc.\n  - Crie gráficos (ex.: histogramas, nuvens de palavras) para insights visuais.\n- Limpeza de dados:\n  - Remova stopwords, caracteres especiais, e transforme o texto para lowercase.\n  - Aborde valores ausentes ou inconsistências nos dados.\n- Plano de análise:\n  - Justifique como a limpeza e exploração guiarão o próximo passo de processamento.\n\nArquitetura do Modelo\n- Explique sua abordagem para transformar texto em dados estruturados:\n  - Escolha métodos como TF-IDF, embeddings pré-treinados (ex.: GloVe, Word2Vec).\n  - Justifique a escolha com base nas necessidades do projeto.\n- Descreva a arquitetura do modelo:\n  - Use redes neurais sequenciais como LSTM, GRU ou Transformers.\n  - Argumente por que sua arquitetura atende ao desafio de classificar textos curtos como tweets.\nResultados e Análise\n- Documente os experimentos:\n  - Teste diferentes modelos e hiperparâmetros.\n  - Compare o impacto de técnicas como dropout, early stopping, e embeddings.\n  - Visualize os resultados (matriz de confusão, precisão, recall, F1-score).\n- Discuta:\n  - Quais estratégias ajudaram ou não?\n  - Quais melhorias poderiam ser feitas?\n Conclusão \n- Resuma as principais conclusões.\n- Discuta limitações e possíveis melhorias futuras.\n- Explique o que você aprendeu no processo.\n\n","metadata":{}},{"cell_type":"code","source":"# Importação de bibliotecas principais\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# NLP e pré-processamento\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Modelos e métricas\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Configuração inicial\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carregar os dados do Kaggle\ntrain_data = pd.read_csv(\"train.csv\")\ntest_data = pd.read_csv(\"test.csv\")\n\n# Visualizar os primeiros registros\nprint(\"Treinamento:\")\nprint(train_data.head())\nprint(\"\\nTeste:\")\nprint(test_data.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Informações gerais\nprint(\"Informações gerais:\")\nprint(train_data.info())\n\n# Distribuição das classes\nsns.countplot(data=train_data, x='target', palette='viridis')\nplt.title(\"Distribuição das Classes (0 = Não Desastre, 1 = Desastre)\")\nplt.show()\n\n# Comprimento dos tweets\ntrain_data['text_length'] = train_data['text'].apply(len)\nsns.histplot(train_data['text_length'], bins=30, kde=True)\nplt.title(\"Distribuição do Comprimento dos Tweets\")\nplt.show()\n\n# Nuvem de palavras\ndisaster_tweets = \" \".join(train_data[train_data['target'] == 1]['text'])\nnon_disaster_tweets = \" \".join(train_data[train_data['target'] == 0]['text'])\n\nplt.figure(figsize=(10, 5))\nwordcloud_disaster = WordCloud(stopwords=stop_words, background_color='white').generate(disaster_tweets)\nplt.imshow(wordcloud_disaster, interpolation='bilinear')\nplt.title(\"Nuvem de Palavras - Tweets de Desastres\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Função para limpar texto\nimport re\n\ndef clean_text(text):\n    text = re.sub(r\"http\\S+\", \"\", text)  # Remover URLs\n    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remover caracteres não alfabéticos\n    text = text.lower()  # Transformar em lowercase\n    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remover stopwords\n    return text\n\n# Aplicar a limpeza\ntrain_data['cleaned_text'] = train_data['text'].apply(clean_text)\ntest_data['cleaned_text'] = test_data['text'].apply(clean_text)\n\nprint(train_data[['text', 'cleaned_text']].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parâmetros\nmax_words = 10000  # Vocabulário máximo\nmax_len = 100  # Comprimento máximo dos tweets\n\n# Tokenização\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_data['cleaned_text'])\n\nX_train = tokenizer.texts_to_sequences(train_data['cleaned_text'])\nX_train = pad_sequences(X_train, maxlen=max_len)\n\n# Rótulos\ny_train = train_data['target']\n\n# Dados de teste\nX_test = tokenizer.texts_to_sequences(test_data['cleaned_text'])\nX_test = pad_sequences(X_test, maxlen=max_len)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construção do modelo\nmodel = Sequential([\n    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n    LSTM(128, return_sequences=True),\n    Dropout(0.2),\n    LSTM(64),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid')\n])\n\n# Compilação\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# Treinamento\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nhistory = model.fit(\n    X_train_split, y_train_split,\n    validation_data=(X_val, y_val),\n    batch_size=32,\n    epochs=10,\n    callbacks=[early_stopping]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Avaliar no conjunto de validação\nval_preds = (model.predict(X_val) > 0.5).astype(int)\nprint(classification_report(y_val, val_preds))\n\n# Fazer previsões no conjunto de teste\ntest_preds = (model.predict(X_test) > 0.5).astype(int)\n\n# Preparar o arquivo para envio no Kaggle\nsubmission = pd.DataFrame({'id': test_data['id'], 'target': test_preds.flatten()})\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Conclusão Final**\nEste projeto foi uma excelente introdução ao mundo do NLP e à aplicação de redes neurais para análise de texto. Com base nos resultados obtidos, sabemos que as técnicas usadas foram eficazes para a tarefa proposta, mas há espaço para melhorias. A exploração de arquiteturas mais complexas e o ajuste fino do modelo têm grande potencial para aumentar a precisão e melhorar a generalização.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}